{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2800cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import pypdf\n",
    "import openai\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from io import StringIO\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import QAGenerationChain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from text_utils import GRADE_DOCS_PROMPT, GRADE_ANSWER_PROMPT, GRADE_DOCS_PROMPT_FAST, GRADE_ANSWER_PROMPT_FAST\n",
    "\n",
    "from gpt_index import (\n",
    "    GPTTreeIndex, \n",
    "    GPTSimpleVectorIndex, \n",
    "    SimpleDirectoryReader, \n",
    "    LLMPredictor, \n",
    "    ServiceContext,\n",
    "    Response\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from gpt_index.evaluation import ResponseEvaluator\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476dac06",
   "metadata": {},
   "source": [
    "`Load docs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29160461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(files):\n",
    "\n",
    "    # Load docs\n",
    "    # IN: List of upload files (from Streamlit)\n",
    "    # OUT: str\n",
    "    # TODO: Support multple docs, Use Langchain loader\n",
    "\n",
    "    all_text = \"\"\n",
    "    for file_path in files:\n",
    "        file_extension = os.path.splitext(file_path)[1]\n",
    "        if file_extension == \".pdf\":\n",
    "            pdf_reader = pypdf.PdfReader(file_path)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "            all_text += text\n",
    "        elif file_extension == \".txt\":\n",
    "            loader = UnstructuredFileLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            all_text += docs[0].page_content\n",
    "        else:\n",
    "            print('Please provide txt or pdf.')\n",
    "\n",
    "    return all_text\n",
    "\n",
    "fis = glob.glob(\"docs/transformers/*pdf\")\n",
    "# fis = glob.glob(\"docs/karpathy-lex-pod/*txt\")\n",
    "text = load_docs(fis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700134c",
   "metadata": {},
   "source": [
    "`Split` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dac8096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Splitting doc ...`\n"
     ]
    }
   ],
   "source": [
    "def split_texts(text, chunk_size, overlap, split_method):\n",
    "\n",
    "    # Split text\n",
    "    # IN: text, chunk size, overlap\n",
    "    # OUT: list of str splits\n",
    "    # TODO: Add parameter for splitter type\n",
    "\n",
    "    print(\"`Splitting doc ...`\")\n",
    "    if split_method == \"RecursiveTextSplitter\":\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                                       chunk_overlap=overlap)\n",
    "    elif split_method == \"CharacterTextSplitter\":\n",
    "        text_splitter = CharacterTextSplitter(separator=\" \",\n",
    "                                              chunk_size=chunk_size,\n",
    "                                              chunk_overlap=overlap)\n",
    "    splits = text_splitter.split_text(text)\n",
    "    return splits\n",
    "\n",
    "split_method = \"RecursiveTextSplitter\" \n",
    "overlap = 100\n",
    "chunk_size = 1000\n",
    "splits = split_texts(text, chunk_size, overlap, split_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae0deb",
   "metadata": {},
   "source": [
    "`Llama Index retriver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52da5129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 823050 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index import Document\n",
    "from gpt_index import (GPTSimpleVectorIndex)\n",
    "documents = [Document(t) for t in splits]\n",
    "# *** How are chunks used since we've passed in a list of chunked docs? ***\n",
    "context = ServiceContext.from_defaults(chunk_size_limit=512)\n",
    "# *** What vector DB implentation? ***\n",
    "vector_index = GPTSimpleVectorIndex.from_documents(documents, service_context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "343cd774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 289 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 11 tokens\n"
     ]
    }
   ],
   "source": [
    "# ***  Can I specify k? ***\n",
    "query = \"What corpus of data was GPT-3 trained on\"\n",
    "answer = vector_index.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f8f9ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGPT-3 was trained on a large web corpus.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42be4ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=Node(text='achieves strong performance on many NLP datasets, including translation, question-answering, and\\ncloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as\\nunscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\\ntime, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some\\ndatasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\\nwe ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty\\ndistinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding\\nand of GPT-3 in general.\\n\\x03Equal contribution\\nyJohns Hopkins University, OpenAI\\nAuthor contributions listed at end of paper.arXiv:2005.14165v4  [cs.CL]  22 Jul 2020Contents\\n1 Introduction 3\\n2 Approach 6', doc_id='a815d4c1-2d1f-43ab-83eb-9c4d1facc672', embedding=None, doc_hash='c448e2f34f36b445afb2da8e3d490ed164825aa5744c56caca343b59f20c79a9', extra_info=None, node_info={'start': 0, 'end': 894}, relationships={<DocumentRelationship.SOURCE: '1'>: '9dd22c29-55cf-4d1a-9233-4ff881960662'}), score=0.880567815653455)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.source_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3929a",
   "metadata": {},
   "source": [
    "`Llama Index eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d785f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/site-packages/gpt_index/data_structs/node_v2.py:170: UserWarning: .source_text is deprecated, use .node.get_text() instead\n",
      "  warnings.warn(\".source_text is deprecated, use .node.get_text() instead\")\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 505 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'YES'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This mode of evaluation will return “YES”/”NO” if the synthesized response matches any source context.\n",
    "evaluator = ResponseEvaluator(service_context=context)\n",
    "eval_result = evaluator.evaluate(answer)\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf425ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 505 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['YES']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This mode of evaluation will return “YES”/”NO” for every source node.\n",
    "eval_result = evaluator.evaluate_source_nodes(answer)\n",
    "pct = eval_result.count('YES') / len(eval_result)\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6351cf",
   "metadata": {},
   "source": [
    "`SVMretriver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191d1c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "class SVMretriver:\n",
    "    \n",
    "    def __init__(self, splits, embeddings):\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "        self.splits = splits \n",
    "        self.embedded_splits = np.array([self.embeddings.embed_query(split) for split in self.splits])\n",
    "                            \n",
    "    def get_relevant_documents(self, question, k):\n",
    "        \n",
    "        query = np.array(self.embeddings.embed_query(question))\n",
    "        x = np.concatenate([query[None, ...], self.embedded_splits])\n",
    "        y = np.zeros(x.shape[0])\n",
    "        y[0] = 1\n",
    "        \n",
    "        clf = svm.LinearSVC(class_weight='balanced', verbose=False, max_iter=10000, tol=1e-6, C=0.1)\n",
    "        clf.fit(x, y)\n",
    "        \n",
    "        similarities = clf.decision_function(x)\n",
    "        sorted_ix = np.argsort(-similarities)\n",
    "        \n",
    "        top_k_results = []\n",
    "        for row in sorted_ix[1:k+1]:\n",
    "            top_k_results.append(Document(page_content=self.splits[row]))\n",
    "        return top_k_results\n",
    "    \n",
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "fis = glob.glob(\"docs/karpathy-lex-pod/*txt\")\n",
    "text = load_docs(fis)\n",
    "svm_retriever = SVMretriver(splits,embeddings)\n",
    "top_k_docs = svm_retriever.get_relevant_documents(question, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657cfa2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36258b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75486fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db22ee9e",
   "metadata": {},
   "source": [
    "`Make retriver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b193b768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Making retriever ...`\n"
     ]
    }
   ],
   "source": [
    "def make_retriever(splits, retriever_type, embeddings, num_neighbors):\n",
    "\n",
    "    # Make document retriever\n",
    "    # IN: list of str splits, retriever type, embedding type, number of neighbors for retrieval\n",
    "    # OUT: retriever\n",
    "\n",
    "    print(\"`Making retriever ...`\")\n",
    "    # Set embeddings\n",
    "    if embeddings == \"OpenAI\":\n",
    "        embd = OpenAIEmbeddings()\n",
    "    elif embeddings == \"HuggingFace\":\n",
    "        embd = HuggingFaceEmbeddings()\n",
    "\n",
    "    # Select retriever\n",
    "    if retriever_type == \"similarity-search\":\n",
    "        try:\n",
    "            vectorstore = FAISS.from_texts(splits, embd)\n",
    "        except ValueError:\n",
    "            print(\"`Error using OpenAI embeddings (disallowed TikToken token in the text). Using HuggingFace.`\", icon=\"⚠️\")\n",
    "            vectorstore = FAISS.from_texts(splits, HuggingFaceEmbeddings())\n",
    "        retriever = vectorstore.as_retriever(k=num_neighbors)\n",
    "    elif retriever_type == \"SVM\":\n",
    "        retriever = SVMRetriever.from_texts(splits,embd)\n",
    "    elif retriever_type == \"TF-IDF\":\n",
    "        retriever = TFIDFRetriever.from_texts(splits)\n",
    "    return retriever\n",
    "\n",
    "retriever_type = \"similarity-search\"\n",
    "embeddings = \"OpenAI\"\n",
    "num_neighbors = 3\n",
    "retriever = make_retriever(splits, retriever_type, embeddings, num_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e61db2",
   "metadata": {},
   "source": [
    "`Make chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6400e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chain(model_version, retriever):\n",
    "\n",
    "    # Make chain\n",
    "    # IN: model version, retriever\n",
    "    # OUT: chain\n",
    "\n",
    "    if (model_version == \"gpt-3.5-turbo\") or (model_version == \"gpt-4\"):\n",
    "        llm = ChatOpenAI(model_name=model_version, temperature=0)\n",
    "    elif model_version == \"anthropic\":\n",
    "        llm = Anthropic(temperature=0)\n",
    "    qa = RetrievalQA.from_chain_type(llm,\n",
    "                                     chain_type=\"stuff\",\n",
    "                                     retriever=retriever,\n",
    "                                     input_key=\"question\")\n",
    "    return qa\n",
    "\n",
    "model_version = \"gpt-3.5-turbo\"\n",
    "qa_chain = make_chain(model_version,retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e524961",
   "metadata": {},
   "source": [
    "`Generate eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "017f0b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Generating eval set ...`\n",
      "`Running eval ...`\n",
      "`Grading model answer ...`\n",
      "`Grading relevance of retrived docs ...`\n"
     ]
    }
   ],
   "source": [
    "def grade_model_answer(predicted_dataset, predictions, grade_answer_prompt):\n",
    "\n",
    "    # Grade the distilled answer\n",
    "    # IN: ground truth, model predictions\n",
    "    # OUT: list of scores\n",
    "\n",
    "    print(\"`Grading model answer ...`\")\n",
    "    if grade_answer_prompt == \"Fast\":\n",
    "        prompt = GRADE_ANSWER_PROMPT_FAST\n",
    "    else:\n",
    "        prompt = GRADE_ANSWER_PROMPT\n",
    "\n",
    "    eval_chain = QAEvalChain.from_llm(llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0), \n",
    "                                      prompt=prompt)\n",
    "    graded_outputs = eval_chain.evaluate(predicted_dataset,\n",
    "                                         predictions,\n",
    "                                         question_key=\"question\",\n",
    "                                         prediction_key=\"result\")\n",
    "    return graded_outputs\n",
    "\n",
    "def grade_model_retrieval(gt_dataset, predictions, grade_docs_prompt):\n",
    "    \n",
    "    # Grade the docs retrieval\n",
    "    # IN: ground truth, model predictions\n",
    "    # OUT: list of scores\n",
    "\n",
    "    print(\"`Grading relevance of retrived docs ...`\")\n",
    "    if grade_docs_prompt == \"Fast\":\n",
    "        prompt = GRADE_DOCS_PROMPT_FAST\n",
    "    else:\n",
    "        prompt = GRADE_DOCS_PROMPT\n",
    "\n",
    "    eval_chain = QAEvalChain.from_llm(llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0), \n",
    "                                      prompt=prompt)\n",
    "    graded_outputs = eval_chain.evaluate(gt_dataset,\n",
    "                                         predictions,\n",
    "                                         question_key=\"question\",\n",
    "                                         prediction_key=\"result\")\n",
    "    return graded_outputs\n",
    "\n",
    "def run_eval(chain, retriever, eval_qa_pair, grade_prompt):\n",
    "\n",
    "    # Compute eval\n",
    "    # IN: chain, retriever, eval question, flag for docs retrieval prompt\n",
    "    # OUT: list of scores for answers and retrival, latency, predictions\n",
    "\n",
    "    print(\"`Running eval ...`\")\n",
    "    predictions = []\n",
    "    retrived_docs = []\n",
    "    gt_dataset = []\n",
    "    latency = []\n",
    "    \n",
    "    # Get answer and log latency\n",
    "    start_time = time.time()\n",
    "    predictions.append(chain(eval_qa_pair))\n",
    "    gt_dataset.append(eval_qa_pair)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    latency.append(elapsed_time)\n",
    "\n",
    "    # Retrive data\n",
    "    docs=retriever.get_relevant_documents(eval_qa_pair[\"question\"])\n",
    "\n",
    "    # Extract text from retrived docs\n",
    "    retrived_doc_text = \"\"\n",
    "    for i,doc in enumerate(docs):\n",
    "        retrived_doc_text += \"Doc %s: \"%str(i+1) + doc.page_content + \" \"\n",
    "    retrived = {\"question\": eval_qa_pair[\"question\"], \"answer\": eval_qa_pair[\"answer\"], \"result\": retrived_doc_text}\n",
    "    retrived_docs.append(retrived)\n",
    "        \n",
    "    # Grade\n",
    "    graded_answers = grade_model_answer(gt_dataset, predictions, grade_prompt)\n",
    "    graded_retrieval = grade_model_retrieval(gt_dataset, retrived_docs, grade_prompt)\n",
    "    return graded_answers, graded_retrieval, latency, predictions\n",
    "\n",
    "def generate_eval(text, N, chunk):\n",
    "\n",
    "    # Generate N questions from context of chunk chars\n",
    "    # IN: text, N questions, chunk size to draw question from in the doc\n",
    "    # OUT: list of JSON\n",
    "    # TODO: Refactor, PR for Langchain repo, add error handling for JSON load in QA chain\n",
    "\n",
    "    print(\"`Generating eval set ...`\")\n",
    "    n = len(text)\n",
    "    starting_indices = [random.randint(0, n-chunk) for _ in range(N)]\n",
    "    sub_sequences = [text[i:i+chunk] for i in starting_indices]\n",
    "    chain = QAGenerationChain.from_llm(ChatOpenAI(temperature=0))\n",
    "    eval_set = []\n",
    "    for i, b in enumerate(sub_sequences):\n",
    "        try:\n",
    "            qa = chain.run(b)\n",
    "            eval_set.append(qa)\n",
    "        except:\n",
    "            print(\"Error on question %s\"%i)\n",
    "    eval_pair = list(itertools.chain.from_iterable(eval_set))\n",
    "    return eval_pair\n",
    "\n",
    "num_eval_questions = 1\n",
    "grade_prompt = \"Fast\"\n",
    "json_input = True\n",
    "\n",
    "if json_input:\n",
    "    with open(\"docs/karpathy-lex-pod/karpathy-pod-eval.json\") as f:\n",
    "        eval_set_full = json.load(f)\n",
    "        num_eval_questions = len(eval_set_full)\n",
    "\n",
    "for i in range(num_eval_questions):\n",
    "    \n",
    "\n",
    "    # Generate one question\n",
    "    if json_input:\n",
    "        eval_pair = eval_set_full[i]\n",
    "    else:\n",
    "        eval_pair = generate_eval(text, 1, 3000)\n",
    "    if len(eval_pair) == 0:  # Error in eval generation\n",
    "        continue\n",
    "    else:\n",
    "        # This returns a list, so we unpack to dict\n",
    "        eval_pair = eval_pair[0]\n",
    "        # Run eval \n",
    "        graded_answers, graded_retrieval, latency, predictions = run_eval(qa_chain, retriever, eval_pair, grade_prompt)\n",
    "        # Assemble output \n",
    "        d = pd.DataFrame(predictions)\n",
    "        d['answer score'] = [g['text'] for g in graded_answers]\n",
    "        d['docs score'] = [g['text'] for g in graded_retrieval]\n",
    "        d['latency'] = latency\n",
    "        # Summary statistics\n",
    "        d['answer correct'] = [\"TRUE\" if \"INCORRECT\" not in text else \"FALSE\" for text in d['answer score']]\n",
    "        d['docs relevant'] = [\"TRUE\" if \"Context is relevant: True\" in text else \"FALSE\" for text in d['docs score']]\n",
    "        # Convert the dictionary to a JSON string\n",
    "        out_data = {'tableA': d.to_dict('records')}\n",
    "        json_str = json.dumps(out_data)\n",
    "        return {\"output dataframe\": json_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f235aca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"tableA\": [{\"question\": \"What is the potential downside of having additional sensors in a car?\", \"answer\": \"Additional sensors can be a liability because they require a supply chain, maintenance, firmware, and can hold back the production line.\", \"result\": \"According to the context, the potential downside of having additional sensors in a car is that they are not free and can be a liability. They require computer vision engineers, procurement, maintenance, firmware writing, and incorporation into the system. They can also contribute noise and entropy into everything, bloat the data engine, and hold back the line in production. Therefore, it is important to consider the full cost of any one sensor and be really sure if it is necessary. In some cases, the cost is high and not worth it.\", \"answer score\": \"CORRECT\", \"docs score\": \"Context is relevant: True.\", \"latency\": 4.297659158706665, \"answer correct\": \"TRUE\", \"docs relevant\": \"TRUE\"}]}'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21372386",
   "metadata": {},
   "source": [
    "`Test SVM retiever`\n",
    "\n",
    "https://github.com/karpathy/randomfun/blob/master/knn_vs_svm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4de93f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Single-process---\n",
      "---Single-process---\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "print(\"---Single-process---\")\n",
    "start_time = time.time()\n",
    "out = np.array([embeddings.embed_query(split) for split in splits])\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)\n",
    "print(\"---Single-process---\")\n",
    "# 758.6700360774994 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5757ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DF---\n",
      "1141.9887909889221\n",
      "---DF---\n"
     ]
    }
   ],
   "source": [
    "print(\"---DF---\")\n",
    "start_time = time.time()\n",
    "d=pd.DataFrame()\n",
    "d['splits']=splits\n",
    "d['embeddings']=d['splits'].apply(lambda x: embeddings.embed_query(x))\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)\n",
    "print(\"---DF---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c2c66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-55:\n",
      "Process SpawnPoolWorker-56:\n",
      "Process SpawnPoolWorker-53:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-50:\n",
      "Process SpawnPoolWorker-52:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-54:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-49:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-51:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-57:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-58:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-59:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-60:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-61:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-62:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-64:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-63:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-65:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-66:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-67:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-68:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-69:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-70:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-71:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-72:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-73:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-74:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-75:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-76:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-77:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-78:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-79:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-80:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'embed_query' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-85:\n",
      "Process SpawnPoolWorker-86:\n",
      "Process SpawnPoolWorker-84:\n",
      "Process SpawnPoolWorker-81:\n",
      "Process SpawnPoolWorker-87:\n",
      "Process SpawnPoolWorker-46:\n",
      "Process SpawnPoolWorker-88:\n",
      "Process SpawnPoolWorker-48:\n",
      "Process SpawnPoolWorker-83:\n",
      "Process SpawnPoolWorker-47:\n",
      "Process SpawnPoolWorker-44:\n",
      "Process SpawnPoolWorker-41:\n",
      "Process SpawnPoolWorker-45:\n",
      "Process SpawnPoolWorker-82:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process SpawnPoolWorker-42:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process SpawnPoolWorker-43:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pr"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m----> 6\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ocess.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "def embed_query(split):\n",
    "    return embeddings.embed_query(split)\n",
    "start_time = time.time()\n",
    "with Pool() as p:\n",
    "    out = np.array(p.map(embed_query, splits))\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc5040e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "758.6700360774994"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a244634",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---Multi-process---\")\n",
    "start_time = time.time()\n",
    "def embed_chunk(chunk):\n",
    "    return embeddings.embed_query(chunk)\n",
    "# Multiprocessing pool with all available CPU cores\n",
    "num_processes = multiprocessing.cpu_count()  \n",
    "pool = multiprocessing.Pool(num_processes)\n",
    "results = pool.map(embed_chunk, splits)\n",
    "pool.close()\n",
    "pool.join()\n",
    "out = np.array(results)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)\n",
    "print(\"---Multi-process---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "95259804",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9cc14afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Software 2.0 refers to the idea that the way we program computers is changing from people writing software in traditional programming languages like C++ to accumulating training sets and data sets and crafting objectives by which we train neural networks. The neural networks are then compiled into a binary, which is the neural net weights and the forward pass of the neural net. This type of programming is also known as machine learning or artificial intelligence programming.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "chain.run(input_documents=top_k_docs, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56366bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "989569b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_retriever(splits, retriever_type, embeddings, num_neighbors):\n",
    "\n",
    "    # Make retriever\n",
    "    # IN: list of str splits, retriever type, and embedding type\n",
    "    # OUT: retriever\n",
    "\n",
    "    if retriever_type == \"similarity-search\":\n",
    "        if embeddings == \"OpenAI\":\n",
    "            vectorstore = FAISS.from_texts(splits, OpenAIEmbeddings())\n",
    "        elif embeddings == \"HuggingFace\":\n",
    "            vectorstore = FAISS.from_texts(splits, HuggingFaceEmbeddings())\n",
    "        retriever = vectorstore.as_retriever(k=num_neighbors)\n",
    "    elif retriever_type == \"TF-IDF\":\n",
    "        retriever = TFIDFRetriever.from_texts(splits, k=num_neighbors)\n",
    "    return retriever\n",
    "\n",
    "def make_chain(model_version, retriever):\n",
    "\n",
    "    # Make chain\n",
    "    # IN: model version, vectorstore\n",
    "    # OUT: chain\n",
    "    # TODO: Support for multiple model types\n",
    "\n",
    "    if (model_version == \"gpt-3.5-turbo\") or (model_version == \"gpt-4\"):\n",
    "        llm = ChatOpenAI(model_name=model_version, temperature=0)\n",
    "    \n",
    "    elif model_version == \"Anthropic\":\n",
    "        llm = Anthropic(temperature=0)\n",
    "    qa = RetrievalQA.from_chain_type(llm,\n",
    "                                     chain_type=\"stuff\",\n",
    "                                     retriever=retriever,\n",
    "                                     input_key=\"question\")\n",
    "    return qa\n",
    "\n",
    "retriever_type = \"similarity-search\"\n",
    "embeddings = \"OpenAI\"\n",
    "num_neighbors = 3\n",
    "retriever = make_retriever(splits, retriever_type, embeddings, num_neighbors)\n",
    "\n",
    "model_version = \"gpt-3.5-turbo\"\n",
    "qa_chain = make_chain(model_version,retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fe0621",
   "metadata": {},
   "source": [
    "`Run eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a286d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are a teacher grading a quiz. You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\"\"\"\n",
    "\n",
    "GRADE_ANSWER_PROMPT = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n",
    "\n",
    "# Citation\n",
    "# https://github.com/jerryjliu/llama_index/blob/main/examples/test_wiki/TestNYC-Benchmark-GPT4.ipynb\n",
    "template = \"\"\" \n",
    "    Given the question below. \\n\n",
    "    ---------------------\\n\n",
    "    {query}\n",
    "    \\n---------------------\\n\n",
    "    Decide if the following retreived context is relevant. \\n\n",
    "    \\n---------------------\\n\n",
    "    {result}\n",
    "    Answer in the following format:\\n\n",
    "    'Context is relevant: True or False'\n",
    "    and explain why it supports the correct answer: {answer}\"\"\"\n",
    "\n",
    "GRADE_DOCS_PROMPT = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b81c8a3",
   "metadata": {},
   "source": [
    "`QAEvalChain` expected `input_variables=[\"query\", \"result\", \"answer\"]`\n",
    " \n",
    "TODO: Make this more general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41896b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Running eval ...`\n",
      "`Grading model answer ...`\n",
      "`Grading relevance of retrived docs ...`\n"
     ]
    }
   ],
   "source": [
    "def grade_model_answer(gt_dataset, predictions):\n",
    "\n",
    "    # Grade the model\n",
    "    # IN: model predictions and ground truth (lists)\n",
    "    # OUT: list of scores\n",
    "    # TODO: Support for multiple grading types\n",
    "\n",
    "    print(\"`Grading model answer ...`\")\n",
    "    eval_chain = QAEvalChain.from_llm(llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0), \n",
    "                                      prompt=GRADE_ANSWER_PROMPT)\n",
    "    graded_outputs = eval_chain.evaluate(gt_dataset,\n",
    "                                         predictions,\n",
    "                                         question_key=\"question\",\n",
    "                                         prediction_key=\"result\")\n",
    "    return graded_outputs\n",
    "\n",
    "def grade_model_retrieval(gt_dataset, predictions):\n",
    "    \n",
    "    # Grade the docs retrieval\n",
    "    # IN: model predictions and ground truth (lists)\n",
    "    # OUT: list of scores\n",
    "    # TODO: Support for multiple grading types\n",
    "\n",
    "    print(\"`Grading relevance of retrived docs ...`\")\n",
    "    eval_chain = QAEvalChain.from_llm(llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0), \n",
    "                                      prompt=GRADE_DOCS_PROMPT)\n",
    "    graded_outputs = eval_chain.evaluate(gt_dataset,\n",
    "                                         predictions,\n",
    "                                         question_key=\"question\",\n",
    "                                         prediction_key=\"result\")\n",
    "    return graded_outputs\n",
    "\n",
    "def run_eval(chain, retriever, eval_set):\n",
    "\n",
    "    # Compute eval\n",
    "    # IN: chain and ground truth (lists)\n",
    "    # OUT: list of scores, latenct, predictions\n",
    "    # TODO: Support for multiple grading types\n",
    "\n",
    "    print(\"`Running eval ...`\")\n",
    "    predictions = []\n",
    "    retrived_docs = []\n",
    "    gt_dataset = []\n",
    "    latency = []\n",
    "        \n",
    "    for data in eval_set:\n",
    "        \n",
    "        # Get answer and log latency\n",
    "        start_time = time.time()\n",
    "        predictions.append(chain(data))\n",
    "        gt_dataset.append(data)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        latency.append(elapsed_time)\n",
    "        # Retrive data\n",
    "        docs=retriever.get_relevant_documents(data[\"question\"])\n",
    "        # Extract text from retrived docs\n",
    "        retrived_doc_text = \"\"\n",
    "        for i,doc in enumerate(docs):\n",
    "            retrived_doc_text += \"Doc %s: \"%str(i+1) + doc.page_content + \" \"\n",
    "        retrived = {\"question\": data[\"question\"],\"answer\": data[\"answer\"], \"result\": retrived_doc_text}\n",
    "        retrived_docs.append(retrived)\n",
    "        \n",
    "    # Grade\n",
    "    graded_answers = grade_model_answer(gt_dataset, predictions)\n",
    "    graded_retrieval = grade_model_retrieval(gt_dataset, retrived_docs)\n",
    "    return graded_answers, graded_retrieval, latency, predictions\n",
    "\n",
    "graded_answers, graded_retrieval, latency, predictions = run_eval(qa_chain, retriever, eval_set_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "105f1a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'How many parameters does the largest GLaM model have?',\n",
       "  'answer': 'The largest GLaM model has 1.2T (trillion) parameters.',\n",
       "  'result': 'The largest version of GLaM has 1.2T parameters in total with 64 experts per MoE layer.'},\n",
       " {'question': 'How many experts per layer does the GLaM model have?',\n",
       "  'answer': 'The GLaM model has 64 experts per layer, with each token activating 96.6B parameters.',\n",
       "  'result': 'The GLaM model can have from 1 to 256 experts per MoE layer, as stated in the context: \"We then increase the number of experts in each MoE layer from 1 to 256.\"'},\n",
       " {'question': 'How many parameters, layers, and heads does the LaMDA model have?',\n",
       "  'answer': 'The LaMDA model has 137B params, 64 layers, and 128 heads.',\n",
       "  'result': 'The largest LaMDA model has 137B non-embedding parameters, and it uses a decoder-only Transformer language model with 64 layers, h=128, dk=dv=128, and relative attention as described in T5 [11]. However, the number of heads is not explicitly mentioned in the given context.'},\n",
       " {'question': 'How many tokens is Chinchilla trained on?',\n",
       "  'answer': 'Chinchilla is trained with 1.5T tokens.',\n",
       "  'result': 'Chinchilla is trained on 1.4 trillion tokens.'},\n",
       " {'question': 'What scaling law does the Chinchilla paper propose?',\n",
       "  'answer': 'For every doubling of model size the number of training tokens should also be doubled.',\n",
       "  'result': 'The Chinchilla paper proposes a power-law relationship between the compute budget, model size, and number of training tokens as a scaling law for neural language models.'},\n",
       " {'question': 'How many parameters does the largest Flamingo model have?',\n",
       "  'answer': 'The largest Flamingo model has 80B parameters.',\n",
       "  'result': 'The largest Flamingo model has 80 billion parameters, as stated in Table 5.'},\n",
       " {'question': 'What tasks is the Gato model capable of?',\n",
       "  'answer': 'The Gato model is capable of tasks including robotics stacking tests, image captioning, and Atari.',\n",
       "  'result': 'The Gato model is a generalist agent that can perform multiple tasks, including playing Atari games, captioning images, engaging in dialogue, stacking blocks with a real robot arm, navigating in simulated 3D environments, and following instructions. It is a multi-modal, multi-task, multi-embodiment generalist policy that can decide based on its context whether to output text, joint torques, button presses, or other tokens. It has been evaluated on various benchmarks, including the RGB Stacking benchmark for robotics, and has achieved high performance on many tasks.'},\n",
       " {'question': 'How many parameters does the LLaMA model have?',\n",
       "  'answer': 'The LLaMA model has 65B parameters.',\n",
       "  'result': 'The LLaMA model ranges from 7B to 65B parameters.'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f8fa472",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'CORRECT'},\n",
       " {'text': 'INCORRECT'},\n",
       " {'text': 'CORRECT'},\n",
       " {'text': 'CORRECT'},\n",
       " {'text': 'INCORRECT'},\n",
       " {'text': 'CORRECT'},\n",
       " {'text': 'CORRECT'},\n",
       " {'text': 'CORRECT'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graded_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35ea97e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Context is relevant: True. \\n\\nThe retrieved context directly answers the question by stating that the largest GLaM model has 1.2T parameters. It provides specific details about the GLaM models and their sizes, including the number of activated parameters per token. Therefore, the context is relevant to the question.'},\n",
       " {'text': 'Context is relevant: True. \\n\\nThe retrieved context mentions the GLaM model and its use of sparsely activated Mixture-of-Experts (MoE) layers. It also specifically states that the largest version of GLaM has 64 experts per MoE layer, with each token activating a subnetwork of 96.6B parameters. Therefore, the context supports the answer that the GLaM model has 64 experts per layer.'},\n",
       " {'text': 'Context is relevant: True\\n\\nThe relevant context from Doc 1 states that the largest LaMDA model has 137B non-embedding parameters, 64 layers, and h=128 heads. Therefore, the context provides the exact information required to answer the question.'},\n",
       " {'text': 'Context is relevant: True. \\n\\nThe retrieved context provides information about Chinchilla, including its training details and the number of tokens it was trained on, which is 1.4T. This directly answers the question of how many tokens Chinchilla was trained on, making the context relevant.'},\n",
       " {'text': 'Context is not relevant. None of the retrieved documents mention the scaling law proposed by the Chinchilla paper.'},\n",
       " {'text': 'Context is relevant: True.\\n\\nThe context mentions the different Flamingo models and their parameter counts, including the largest model referred to as simply Flamingo, which has 80B parameters. Therefore, the context directly answers the question about the parameter count of the largest Flamingo model.'},\n",
       " {'text': 'Context is relevant: True\\n\\nThe retrieved context provides information about the capabilities of the Gato model, stating that it can perform tasks such as robotics stacking tests, image captioning, and playing Atari games. This directly answers the question of what tasks the Gato model is capable of, making the context relevant.'},\n",
       " {'text': 'Context is relevant: True. \\n\\nThe retrieved context mentions LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. It also states that LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. Therefore, the context supports the answer that the LLaMA model has multiple parameters, including a 65B parameter model.'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graded_retrieval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
